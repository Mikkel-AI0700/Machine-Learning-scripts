{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the required dependencies using dependency importer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import importlib\n",
    "from typing import List, Dict, Union, Callable\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Logger configuration\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "class ImportRequiredDependencies:\n",
    "    def import_through_selection (\n",
    "        self, standard_module: bool = False, \n",
    "        sklearn_module: bool = False, \n",
    "        package_module: str = None, \n",
    "        modules_to_import: List[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dynamically imports modules. Inserts dynamically imported modules inside globals()\n",
    "\n",
    "        Parameters:\n",
    "            standard_module (bool): Set to True if importing modules not inside packages\n",
    "            sklearn_module (bool): Set to True if importing modules from Scikit-Learn\n",
    "            module (str): Scikit-Learn module to get attribute from\n",
    "            modules_to_import (Dict[str, str]): Modules containing the key-value modules to import\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if standard_module:\n",
    "                for module in modules_to_import:\n",
    "                    logging.info(\"[*] Importing module: {}\".format(module))\n",
    "                    globals()[module] = importlib.import_module(module)\n",
    "            if sklearn_module:\n",
    "                for module in modules_to_import:\n",
    "                    logging.info(\"[*] Importing {}\".format(module))\n",
    "                    globals()[module] = getattr(importlib.import_module(package_module), module)\n",
    "        except ModuleNotFoundError as non_existent_module:\n",
    "            logging.error(\"[!] Error: {}\".format(non_existent_module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[*] Importing module: numpy\n",
      "INFO:root:[*] Importing module: pandas\n"
     ]
    }
   ],
   "source": [
    "importer_one_params = {\n",
    "    \"standard_module\" : True,\n",
    "    \"modules_to_import\": [\"numpy\", \"pandas\"]\n",
    "}\n",
    "\n",
    "importer = ImportRequiredDependencies()\n",
    "importer.import_through_selection(**importer_one_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# WARNING: DO NOT IMPORT THE MODULES ABOVE THE CLASS. USE DEPENDENCY IMPORTER\n",
    "\n",
    "from typing import List, Dict, Union\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import numpy\n",
    "import pandas\n",
    "import ucimlrepo\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "class LoadDataset:\n",
    "    \"\"\"\n",
    "    Dataset loader that will load and create three datasets\n",
    "\n",
    "    This class will either take in a UCI Machine Learning repository ID or filesystem path\n",
    "    to load and create three datasets: main dataframe, copy of main dataset, a numpy representation\n",
    "    of the original dataset\n",
    "\n",
    "    Parameters:\n",
    "        uci_id (int): ID of ucimlrepo dataset that will be used to get the dataset\n",
    "        load_method (str): Option that will get a function reference that will load the dataset\n",
    "        filesystem_path (str): Filesystem path that points to the dataset file\n",
    "    \"\"\"\n",
    "    def __init__ (self, uci_id: int = None, load_method: str = None, filesystem_path: str = None, **kwargs):\n",
    "        self.uci_id = uci_id\n",
    "        self.loader_method = load_method\n",
    "        self.fs_path = filesystem_path\n",
    "        self.extra_params = kwargs\n",
    "        self.datasets = {}\n",
    "        self.loader_methods = {\n",
    "            \"csv\": pandas.read_csv,\n",
    "            \"xlsx\": pandas.read_excel,\n",
    "            \"json\": pandas.read_json,\n",
    "            \"pickle\": pandas.read_pickle,\n",
    "            \"uci\": ucimlrepo.fetch_ucirepo\n",
    "        }\n",
    "\n",
    "    def _get_loading_method (self):\n",
    "        \"\"\"\n",
    "        Get loader reference if loader_method property exists in loader_methods property keys\n",
    "\n",
    "        Parameters:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            Pandas dataframe reader or UCI Machine Learning Repository reader\n",
    "        \"\"\"\n",
    "        if self.loader_method in self.loader_methods.keys():\n",
    "            return self.loader_methods.get(self.loader_method)\n",
    "\n",
    "    def _load_pandas (self):\n",
    "        \"\"\"\n",
    "        Will get loader reference and if filesystem paths exists, it will assign/create\n",
    "        three datasets: main dataframe, copy of main dataframe and a numpy representation of main dataframe\n",
    "\n",
    "        Parameters:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            datasets (dict): Three datasets, two pandas dataframe and one numpy representation\n",
    "        \"\"\"\n",
    "        loader = self._get_loading_method()\n",
    "\n",
    "        if os.path.exists(self.fs_path):\n",
    "            self.datasets = {\n",
    "                \"main_df\": loader(self.fs_path, **self.extra_params),\n",
    "                \"copy_df\": loader(self.fs_path, **self.extra_params).copy(),\n",
    "                \"numpy_df\": loader(self.fs_path, **self.extra_params).to_numpy()\n",
    "            }\n",
    "            return self.datasets\n",
    "\n",
    "    def _load_uci (self):\n",
    "        \"\"\"\n",
    "        Will load a temporary raw dataset from UCI Machine Learning Repository, then it will\n",
    "        assign/create three datasets\n",
    "\n",
    "        Parameters:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            datasets (dict): Three datasets, two pandas dataframe and one numpy representation\n",
    "        \"\"\"\n",
    "        loader = self._get_loading_method()\n",
    "        temporary_dataset = loader(id=self.uci_id)\n",
    "\n",
    "        self.datasets = {\n",
    "            \"main_df\": pandas.DataFrame(temporary_dataset.data.original, **self.extra_params),\n",
    "            \"copy_df\": pandas.DataFrame(temporary_dataset.data.original, **self.extra_params).copy(),\n",
    "            \"numpy_df\": pandas.DataFrame(temporary_dataset.data.original, **self.extra_params).to_numpy()\n",
    "        }\n",
    "        return self.datasets\n",
    "\n",
    "    def load (self, use_uci: bool = False, use_pandas: bool = False):\n",
    "        \"\"\"\n",
    "        Method that allows the user to choose wether to load datasets via Pandas or ucimlrepo\n",
    "\n",
    "        Parameters:\n",
    "            use_uci (bool): Set to True if datasets need to be loaded using ucimlrepo\n",
    "            use_pandas (bool): Set to True if datasets need to be loaded using Pandas\n",
    "\n",
    "        Returns:\n",
    "            datasets (dict): Three datasets, two pandas dataframe and one numpy representation\n",
    "        \"\"\"\n",
    "        if use_uci:\n",
    "            logging.info(\"[*] Creating three datasets using pandas\")\n",
    "            dataset_dictionary = self._load_uci()\n",
    "        if use_pandas:\n",
    "            logging.info(\"[*] Creating three datasets using ucimlrepo\")\n",
    "            dataset_dictionary = self._load_pandas()\n",
    "\n",
    "        return dataset_dictionary\n",
    "\n",
    "    def reset_datasets (self, dataset_dict: Dict[str, Union[numpy.ndarray, pandas.DataFrame]] = None):\n",
    "        \"\"\"\n",
    "        Method will reset the datasets dictionary should the datasets dictionary gets messed up\n",
    "\n",
    "        Parameters:\n",
    "            dataset_dict (dict): Dictionary that contains the three original datasets\n",
    "\n",
    "        Returns:\n",
    "            dataset_dict (dict): Resetted dictionary\n",
    "        \"\"\"\n",
    "        dataset_dict = self.datasets\n",
    "        return dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[*] Creating three datasets using pandas\n"
     ]
    }
   ],
   "source": [
    "loader_params = {\n",
    "    \"uci_id\": 53,\n",
    "    \"load_method\": \"uci\"\n",
    "}\n",
    "\n",
    "loader = LoadDataset(**loader_params)\n",
    "datasets = loader.load(use_uci=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length  sepal width  petal length  petal width           class\n",
       "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
       "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
       "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
       "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
       "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
       "..            ...          ...           ...          ...             ...\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.get(\"main_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:33: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:33: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/tmp/ipykernel_5365/1928823701.py:33: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  if isinstance(dataset, numpy.ndarray) and re.findall(\"(^int\\.[0-9][0-9]|^float\\.[0-9][0-9][0-9])\", dataset.dtypes):\n",
      "ERROR:root:Numpy dataset or Numpy samples datatypes is incorrect\n",
      "ERROR:root:Either scaling_preprocessing_type argument doesn't match or dataset type is incorrect\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# WARNING: DO NOT COPY PASTE THE IMPORTS ABOVE CLASS. USE DEPENDENCY IMPORTER\n",
    "from typing import *\n",
    "import re\n",
    "import logging\n",
    "import numpy\n",
    "import pandas\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, MaxAbsScaler\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "class ScaleColumns (BaseEstimator, TransformerMixin):\n",
    "    def __init__ (\n",
    "        self, \n",
    "        columns_to_preprocess: Union[str, List[str]] = None, \n",
    "        scaler_parameters: Dict[str, Union[str, float, numpy.ndarray, pandas.DataFrame]] = None, \n",
    "        scaling_preprocessing_type: str = None, \n",
    "        numpy_output: bool = False, \n",
    "        pandas_output: bool = False,\n",
    "    ):\n",
    "        self.columns = columns_to_preprocess\n",
    "        self.scaler_type = scaling_preprocessing_type\n",
    "        self.scaler_parameters = scaler_parameters\n",
    "        self.numpy_output = numpy_output\n",
    "        self.pandas_output = pandas_output\n",
    "\n",
    "    def _is_correct_datatype (self, dataset: Union[numpy.ndarray, pandas.DataFrame] = None):\n",
    "        numpy_datatypes = (numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.float16, numpy.float32, numpy.float64)\n",
    "\n",
    "        try:\n",
    "            if isinstance(dataset, numpy.ndarray) and re.findall(\"(^int\\.[0-9][0-9]|^float\\.[0-9][0-9][0-9])\", dataset.dtypes):\n",
    "                logging.info(\"[*] Numpy dataset and samples type is correct\")\n",
    "                return True\n",
    "            else:\n",
    "                raise ValueError(\"Numpy dataset or Numpy samples datatypes is incorrect\")\n",
    "\n",
    "            if isinstance(dataset, pandas.DataFrame) and dataset[[self.columns]].dtypes.isin(numpy_datatypes).all():\n",
    "                logging.info(\"[*] Pandas dataset and samples type is correct\")\n",
    "                return True\n",
    "            else:\n",
    "                raise ValueError(\"Pandas dataset or Pandas samples datatypes is incorrect\")\n",
    "        except ValueError as incorrect_datatypes_error:\n",
    "            logging.error(incorrect_datatypes_error)\n",
    "\n",
    "    def _transform_dataset (\n",
    "        self, \n",
    "        retain_numpy: bool = None, \n",
    "        retain_pandas: bool = None, \n",
    "        scaler: TransformerMixin = None, \n",
    "        dataset: Union[numpy.ndarray, pandas.DataFrame] = None\n",
    "    ):\n",
    "        log_message = \"[*] Scaler: {}\\n[*] Dataset: {}\\n[*] Columns: {}\"\n",
    "\n",
    "        if retain_numpy:\n",
    "            logging.info(log_message.format(scaler.__class__.__name__, dataset, self.columns))\n",
    "            return scaler.fit_transform(dataset)\n",
    "\n",
    "        if retain_pandas:\n",
    "            if scaler.__class__.__name__ == \"Normalizer\":\n",
    "                logging.info(log_message.format(\"Normalizer\", dataset, self.columns))\n",
    "                dataset = pandas.DataFrame(\n",
    "                    scaler.fit_transform(dataset.values), dataset.index, dataset.columns\n",
    "                )\n",
    "            else:\n",
    "                logging.info(log_message.format(scaler.__class__.__name__, dataset, self.columns))\n",
    "                dataset[[self.columns]] = pandas.DataFrame(\n",
    "                    scaler.fit_transform(dataset[[self.columns]]), dataset[[self.columns]].columns\n",
    "                )\n",
    "            return dataset\n",
    "\n",
    "    def fit_transform (self, X, y=None):\n",
    "        scaler_instances = {\n",
    "            \"standard\" : StandardScaler(**(self.scaler_parameters or {})),\n",
    "            \"minmax\" : MinMaxScaler(**(self.scaler_parameters or {})),\n",
    "            \"maxabs\" : MaxAbsScaler(**(self.scaler_parameters or {})),\n",
    "            \"normalizer\" : Normalizer(**(self.scaler_parameters or {}))\n",
    "        }\n",
    "\n",
    "        if self.scaler_type in scaler_instances and self._is_correct_datatype(X):\n",
    "            logging.info(\"[*] Passing dataset and other parameters to scaler function...\")\n",
    "            transformed_dataset = self._transform_dataset(\n",
    "                self.numpy_output, self.pandas_output, scaledatasets.get(\"main_df\")[[\"sepal length\", \"sepal width\"]].dtypes\n",
    "            )\n",
    "            return transformed_dataset\n",
    "        else:\n",
    "            logging.error(\"Either scaling_preprocessing_type argument doesn't match or dataset type is incorrect\")\n",
    "\n",
    "scaler_params = {\n",
    "    \"columns_to_preprocess\": [\"sepal length\", \"sepal width\"],\n",
    "    \"scaling_preprocessing_type\": \"standard\",\n",
    "    \"pandas_output\": True\n",
    "}\n",
    "\n",
    "scaler = ScaleColumns(**scaler_params)\n",
    "datasets.get(\"main_df\")[[\"sepal length\"]] = scaler.fit_transform(datasets.get(\"main_df\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
