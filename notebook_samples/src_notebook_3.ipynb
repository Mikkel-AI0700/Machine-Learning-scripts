{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_gridsearch (training_dataset_x, training_dataset_y, testing_dataset_x, testing_dataset_y, estimator, param_grid, cross_val, scorers, scorer_to_use, **kwargs):\n",
    "    gridsearch_instance = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=cross_val, scoring=scorers, refit=scorer_to_use, **kwargs)\n",
    "\n",
    "    # ----- Starting the model training -----\n",
    "    print(\"[+] Starting to train the model\")\n",
    "    gridsearch_instance.fit(training_dataset_x, training_dataset_y)\n",
    "\n",
    "    # ----- Placing the training results into dictionaries -----\n",
    "    print(\"[+] Model finished training -> {}\".format(estimator))\n",
    "\n",
    "    gridsearch_results_dict = {\n",
    "        \"gs_cv_results\" : gridsearch_instance.cv_results_,\n",
    "        \"gs_best_score\" : gridsearch_instance.best_score_,\n",
    "        \"gs_best_params\" : gridsearch_instance.best_params_,\n",
    "        \"gs_best_estimator\" : gridsearch_instance.best_estimator_\n",
    "    }\n",
    "\n",
    "    scores_for_default_metrics = {\n",
    "        \"accuracy\" : accuracy_score(testing_dataset_y, gridsearch_instance.predict(testing_dataset_x)),\n",
    "        \"balanced_accuracy\" : balanced_accuracy_score(testing_dataset_y, gridsearch_instance.predict(testing_dataset_x)),\n",
    "        \"precision\" : precision_score(testing_dataset_y, gridsearch_instance.predict(testing_dataset_x)),\n",
    "        \"roc_auc_score\" : roc_auc_curve(testing_dataset_y, gridsearch_instance.predict(testing_dataset_x))\n",
    "    }\n",
    "\n",
    "    return gridsearch_results_dict, scores_for_default_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainUsingGridSearch:\n",
    "    def __init__ (self, test_dataset_y, estimator, param_grid, scoring_dictionary, cross_val_method, scorer_to_use, **kwargs):\n",
    "        self.standard_params_dict = {\n",
    "            \"estimator\" : estimator,\n",
    "            \"param_grid\" : param_grid,\n",
    "            \"scoring\" : scoring_dictionary,\n",
    "            \"refit\" : scorer_to_use,\n",
    "            \"cv\" : cross_val_method,\n",
    "            \"n_jobs\" : 110,\n",
    "            \"verbose\" : 3\n",
    "        }\n",
    "        self.gridsearch_attributes = {}\n",
    "        self.default_metrics_scores = {}\n",
    "        self.model_prediction_results = {}\n",
    "\n",
    "    def _distribute_attributes_and_metrics (self, gridsearch_instance, testing_dataset_x, testing_dataset_y):\n",
    "        # ----- Storing key names, function references and attributes in tuples for zip function looping -----\n",
    "        metric_names_and_references = [\n",
    "            (\"accuracy_score\", accuracy_score), \n",
    "            (\"balanced_accuracy_score\", balanced_accuracy_score), \n",
    "            (\"precision_score\", precision_score), \n",
    "            (\"roc_auc_curve\",  roc_auc_curve)\n",
    "        ]\n",
    "        gridsearch_attributes_tuple = [\n",
    "            (\"gridsearch_cv_results\", gridsearch_instance.cv_results_), \n",
    "            (\"best_score\", gridsearch_instance.best_score_), \n",
    "            (\"best_params\", gridsearch_instancebest_params_), \n",
    "            (\"scorer\", gridsearch_instance.scorer_)\n",
    "        ]\n",
    "\n",
    "        # ----- Storing metric scores self.default_metrics_scores -----\n",
    "        temp_model_predictions, temp_model_probability_predictions = gridsearch_instance.predict(testing_dataset_x), gridsearch_instance.predict_proba(testing_dataset_x)\n",
    "        for metric_name_reference in metric_names_and_references:\n",
    "            if metric_name_reference[0] == \"roc_auc_curve\":\n",
    "                self.default_metrics_scores.update({metric_name_reference[0] : metric_name_reference[1](testing_dataset_y, temp_model_probability_predictions)})\n",
    "            else:\n",
    "                self.default_metrics_scores.update({metric_name_reference[0] : metric_name_reference[1](testing_dataset_y, temp_model_predictions)})\n",
    "\n",
    "        # ----- Distributing attributes in self.gridsearch_attributes dictionary -----\n",
    "        for gs_attribute_tuple in gridsearch_attributes_tuple:\n",
    "            self.gridsearch_attributes.update({gs_attribute_tuple[0] : gs_attribute_tuple[1]})\n",
    "\n",
    "        # ----- Storing model predictions and probabilities in self.model_prediction_results -----\n",
    "        self.model_prediction_results.update({\"raw_model_predictions\" : temp_model_predictions})\n",
    "        self.model_prediction_results.update({\"model_probabilities_predictions\" : temp_model_probability_predictions})\n",
    "        \n",
    "    def start_gridsearch_training (self, training_dataset_x, training_dataset_y, test_dataset_x, test_dataset_y):\n",
    "        try:\n",
    "            if any(dataset == None for dataset in [training_dataset_x, training_dataset_y, test_dataset_x, test_dataset_y]):\n",
    "                raise ValueError(\"[-] Error: One of the datasets passed is has the value type of None\")     \n",
    "            else:\n",
    "                # ----- Creating an instance of gridsearch -----\n",
    "                print(\"[+] Instantiating GridSearch\")\n",
    "                gridsearch_instance = GridSearchCV(**self.standard_params_dict)\n",
    "\n",
    "                # ----- Starting GridSearch training -----\n",
    "                print(\"[+] Starting GridSearch training\")\n",
    "                gridsearch_instance.fit(training_dataset_x, training_dataset_y)\n",
    "\n",
    "                # ----- Putting scores into dictionaries then returning the attributes, scores, and predictions -----\n",
    "                self._distribute_attributes_and_metrics(gridsearch_instance, test_dataset_x, test_dataset_y)\n",
    "                return self.gridsearch_attributes, self.default_metrics_scores, self.model_prediction_results\n",
    "        except ValueError as dataset_missing_error:\n",
    "            print(dataset_missing_error)\n",
    "\n",
    "    def reset_model_predictions (self, dictionary_predictions_variable):\n",
    "        dictionary_predictions_variable = self.model_prediction_results\n",
    "        return dictionary_predictions_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__ (self, test_dataset_x, test_dataset_y):\n",
    "        pass\n",
    "\n",
    "    def display_learning_curve (self):\n",
    "        pass\n",
    "\n",
    "    def display_roc_curve (self):\n",
    "        pass\n",
    "\n",
    "    def precision_recall_display (self):\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
